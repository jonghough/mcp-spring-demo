# Ollama Configuration
# Default Ollama base URL. Ensure Ollama is running on this port.
spring.ai.ollama.base-url=http://localhost:11434
# Specify the Ollama model to use (e.g., llama3, mistral, etc.).
# Make sure this model is pulled in your local Ollama instance (e.g., ollama pull llama3).
spring.ai.ollama.chat.model=llama2

# MCP Client Configuration
# Define a connection to your MCP server. 'myMcpServer' is an arbitrary name for this connection.
spring.ai.mcp.client.sse.connections.myMcpServer.url=http://localhost:8080
# The specific endpoint for sending messages to the MCP server.
spring.ai.mcp.client.sse.connections.myMcpServer.sse-message-endpoint=/mcp/messages
server.port=8081